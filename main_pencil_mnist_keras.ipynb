{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "seed = 42\n",
    "np.random.seed(seed)  # for reproducibility\n",
    "random.seed(seed)\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10 # number of categories we classify. MNIST is 10 digits\n",
    "# input image dimensions. In CNN we think we have a \"color\" image with 1 channel of color.\n",
    "# in MLP with flatten the pixels to img_rows*img_cols\n",
    "img_color, img_rows, img_cols = 1, 28, 28\n",
    "img_size = img_color*img_rows*img_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST training data set label distribution [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
      "test distribution [ 980 1135 1032 1010  982  892  958 1028  974 1009]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "# keras has a built in tool that download the MNIST data set for you to `~/.keras/datasets/`\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print('MNIST training data set label distribution', np.bincount(y_train))\n",
    "print('test distribution', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], img_size)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_size)\n",
    "    \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noisy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_LEVEL=0.46  # what part of training labels are permuted\n",
    "perm = np.array([7, 9, 0, 4, 2, 1, 3, 5, 6, 8])  # noise permutation (from Reed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = perm[y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace some of the training labels with permuted (noise) labels.\n",
    "# make sure each categories receive an equal amount of noise\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "_, noise_idx = next(iter(StratifiedShuffleSplit(n_splits=1,\n",
    "                                                test_size=NOISE_LEVEL,\n",
    "                                                random_state=seed).split(X_train,y_train)))\n",
    "y_train_noise = y_train.copy()\n",
    "y_train_noise[noise_idx] = noise[noise_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actual noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45999999999999996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1. - np.mean(y_train_noise == y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split training data to training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the training set to 10% validation which we will use for early stopping.\n",
    "train_idx, val_idx = next(iter(\n",
    "        StratifiedShuffleSplit(n_splits=1, test_size=0.1,\n",
    "                               random_state=seed).split(X_train, y_train_noise)))\n",
    "X_train_train = X_train[train_idx]\n",
    "y_train_train = y_train_noise[train_idx]\n",
    "X_train_val = X_train[val_idx]\n",
    "y_train_val = y_train_noise[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "y_train_train = to_categorical(y_train_train, num_classes=10)\n",
    "y_train_val = to_categorical(y_train_val, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_train, y_train_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_train_val, y_train_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the test dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(500, activation=\"relu\")(inputs)\n",
    "x1 = Activation('relu')(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "x2 = layers.Dense(300, activation=\"relu\")(x1)\n",
    "x2 = Activation('relu')(x2)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "outputs = layers.Dense(10, activation='softmax', name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "test_acc_metric = keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.3761\n",
      "Training loss (for one batch) at step 200: 0.9789\n",
      "Training acc over epoch: 0.4287\n",
      "Validation acc: 0.4778\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.9669\n",
      "Training loss (for one batch) at step 200: 0.8886\n",
      "Training acc over epoch: 0.4820\n",
      "Validation acc: 0.4955\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.0043\n",
      "Training loss (for one batch) at step 200: 0.8839\n",
      "Training acc over epoch: 0.4924\n",
      "Validation acc: 0.5117\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.8125\n",
      "Training loss (for one batch) at step 200: 0.7880\n",
      "Training acc over epoch: 0.4999\n",
      "Validation acc: 0.5062\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.8244\n",
      "Training loss (for one batch) at step 200: 0.7848\n",
      "Training acc over epoch: 0.5041\n",
      "Validation acc: 0.5042\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.8167\n",
      "Training loss (for one batch) at step 200: 0.8006\n",
      "Training acc over epoch: 0.5111\n",
      "Validation acc: 0.5063\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.8051\n",
      "Training loss (for one batch) at step 200: 0.8161\n",
      "Training acc over epoch: 0.5154\n",
      "Validation acc: 0.5130\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.7776\n",
      "Training loss (for one batch) at step 200: 0.7409\n",
      "Training acc over epoch: 0.5172\n",
      "Validation acc: 0.5073\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.7829\n",
      "Training loss (for one batch) at step 200: 0.7931\n",
      "Training acc over epoch: 0.5162\n",
      "Validation acc: 0.5100\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.7698\n",
      "Training loss (for one batch) at step 200: 0.8090\n",
      "Training acc over epoch: 0.5235\n",
      "Validation acc: 0.5108\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.7478\n",
      "Training loss (for one batch) at step 200: 0.7750\n",
      "Training acc over epoch: 0.5248\n",
      "Validation acc: 0.5140\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 0.7836\n",
      "Training loss (for one batch) at step 200: 0.7628\n",
      "Training acc over epoch: 0.5269\n",
      "Validation acc: 0.5137\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 0.7471\n",
      "Training loss (for one batch) at step 200: 0.7974\n",
      "Training acc over epoch: 0.5272\n",
      "Validation acc: 0.5120\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 0.7649\n",
      "Training loss (for one batch) at step 200: 0.7555\n",
      "Training acc over epoch: 0.5282\n",
      "Validation acc: 0.5018\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.7515\n",
      "Training loss (for one batch) at step 200: 0.7566\n",
      "Training acc over epoch: 0.5278\n",
      "Validation acc: 0.5002\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.7175\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "   \n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_cce(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch_test, y_batch_test in test_dataset:\n",
    "    \n",
    "    test_logits = model(x_batch_test, training=False)\n",
    "    # Update val metrics\n",
    "\n",
    "    test_acc_metric.update_state(y_batch_test, test_logits)\n",
    "test_acc = test_acc_metric.result()\n",
    "test_acc_metric.reset_states()\n",
    "print(\"test acc: %.4f\" % (float(test_acc),))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "y_train_train_np = np.copy(y_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "# for x_batch_val, y_batch_val in val_dataset:\n",
    "#     loss1 = -tf.reduce_mean(tf.multiply(tf.nn.softmax(y_batch_val), tf.nn.log_softmax(y_batch_val))).numpy()\n",
    "#     loss2 = cce(y_batch_val,y_batch_val).numpy()\n",
    "#     print(y_batch_val.shape,loss1,loss2)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "   \n",
    "    n_train = X_train_train.shape[0] \n",
    "    for step in range((n_train//batch_size)+1):\n",
    "        end_index = None if batch_size*(step+1)>n_train else batch_size*(step+1) \n",
    "    \n",
    "        x_batch_train = X_train_train[batch_size*step:end_index]\n",
    "        y_batch_train = y_train_train[batch_size*step:end_index]\n",
    "        y_batch_train_np = y_train_train_np[batch_size*step:end_index]\n",
    "        y_batch_train_variable = tf.Variable(y_batch_train_np)\n",
    "       \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "        \n",
    "            loss_entropy = -tf.reduce_mean(tf.multiply(tf.nn.softmax(logits), tf.nn.log_softmax(logits)))\n",
    "            loss_classification = tf.reduce_mean(tf.nn.softmax(logits)*(tf.nn.log_softmax(logits)-tf.math.log((tf.nn.softmax(y_batch_train_variable)))))\n",
    "            loss_compatibility = loss_cce(tf.nn.softmax(y_batch_train_variable),y_batch_train)\n",
    "            loss_forModel = loss_entropy + loss_classification\n",
    "            loss_forVar = loss_classification + loss_compatibility\n",
    "            \n",
    "        \n",
    "        grads = tape.gradient(loss_forModel, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        grads_variable = tape.gradient(loss_forVar, y_batch_train_variable)\n",
    "        y_train_train_np[batch_size*step:end_index] -= 1000*grads_variable.numpy()\n",
    "        \n",
    "        print('\\nepoch: {}, step: {}'.format(epoch, step))\n",
    "        print('\\npredict:{}, \\nlabel: {}'.format(logits[0].numpy(),y_batch_train[0]))\n",
    "        print('\\nentropy_loss:{:4f}, compatibility_loss: {:4f}, classification_loss: {:4f}'.format(loss_entropy.numpy(),loss_compatibility.numpy(),loss_classification.numpy()))\n",
    "        print('\\nvariable:{} \\ngradient: {}'.format(y_batch_train_np[0],grads_variable[0].numpy()))\n",
    "            \n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "    \n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_entropy))\n",
    "            )\n",
    "            \n",
    "   \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch_test, y_batch_test in test_dataset:\n",
    "    \n",
    "    test_logits = model(x_batch_test, training=False)\n",
    "    # Update val metrics\n",
    "\n",
    "    test_acc_metric.update_state(y_batch_test, test_logits)\n",
    "test_acc = test_acc_metric.result()\n",
    "test_acc_metric.reset_states()\n",
    "print(\"test acc: %.4f\" % (float(test_acc),))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "y_train_train_np = np.copy(y_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "   \n",
    "    n_train = X_train_train.shape[0] \n",
    "    for step in range((n_train//batch_size)+1):\n",
    "        end_index = None if batch_size*(step+1)>n_train else batch_size*(step+1) \n",
    "    \n",
    "        x_batch_train = X_train_train[batch_size*step:end_index]\n",
    "        y_batch_train = y_train_train[batch_size*step:end_index]\n",
    "        y_batch_train_np = y_train_train_np[batch_size*step:end_index]\n",
    "       \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_classification = tf.reduce_mean(tf.nn.softmax(logits)*(tf.nn.log_softmax(logits)-tf.math.log((tf.nn.softmax(y_batch_train_np)))))\n",
    "            \n",
    "        grads = tape.gradient(loss_classification, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "    \n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_entropy))\n",
    "            )\n",
    "            \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch_test, y_batch_test in test_dataset:\n",
    "    \n",
    "    test_logits = model(x_batch_test, training=False)\n",
    "    # Update val metrics\n",
    "\n",
    "    test_acc_metric.update_state(y_batch_test, test_logits)\n",
    "test_acc = test_acc_metric.result()\n",
    "test_acc_metric.reset_states()\n",
    "print(\"test acc: %.4f\" % (float(test_acc),))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
